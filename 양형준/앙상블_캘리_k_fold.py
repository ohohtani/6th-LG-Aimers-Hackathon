###ì•™ìƒë¸”(XG+CAT) ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(optuna)###
# í•™ìŠµ ë° ì»¬ë ˆë¸Œë ˆì´ì…˜, ì˜ˆì¸¡ ì‹¤í–‰ì€ ì•™ìƒë¸”_í•™ìŠµ.py

import numpy as np
import pandas as pd
import optuna
import pickle
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder

# ğŸ”¹ 1. ë°ì´í„° ë¡œë“œ
file_path_train = "train3_updated.csv"
file_path_test = "test3_updated.csv"
sample_submission_path = "sample_submission.csv"

df_train = pd.read_csv(file_path_train)
df_test = pd.read_csv(file_path_test)
df_sample_submission = pd.read_csv(sample_submission_path)

# ğŸ”¹ 2. 'ID' ì»¬ëŸ¼ ìœ ì§€ (sample_submissionì„ ìœ„í•´ í•„ìš”)
test_ids = df_sample_submission["ID"]

# ğŸ”¹ 3. Train ë°ì´í„° ì¤€ë¹„
target_col = "ì„ì‹  ì„±ê³µ ì—¬ë¶€"
if target_col not in df_train.columns:
    raise ValueError(f"âŒ '{target_col}' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„° í™•ì¸ í•„ìš”!")

X = df_train.drop(columns=["ID", target_col], errors="ignore")
y = df_train[target_col]

# ğŸ”¹ 4. ë²”ì£¼í˜• ë³€ìˆ˜ í™•ì¸
cat_features = X.select_dtypes(include=["object"]).columns.tolist()

# ğŸ”¹ 5. CATboostë¥¼ ìœ„í•œ objective ë³€ìˆ˜ ë¬¸ìì—´ ë³€í™˜
df_test_cat = df_test.copy()
df_test_cat = df_test_cat.drop(columns=["ID"], errors="ignore")
for col in cat_features:
    df_test_cat[col] = df_test_cat[col].astype(str)
X_cat = X.copy()
for col in cat_features:
    X_cat[col] = X_cat[col].astype(str)

# âœ… **XGBoostë¥¼ ìœ„í•œ ë ˆì´ë¸” ì¸ì½”ë”© (Train & Test í•©ì³ì„œ ì§„í–‰)**
df_train_xgb = df_train.copy()
df_test_xgb = df_test.copy()

combined_df = pd.concat([df_train[cat_features], df_test[cat_features]], axis=0, ignore_index=True)

for col in cat_features:
    le = LabelEncoder()
    combined_df[col] = le.fit_transform(combined_df[col])

df_train_xgb[cat_features] = combined_df.iloc[:len(df_train)][cat_features]
df_test_xgb[cat_features] = combined_df.iloc[len(df_train):][cat_features]

# âœ… Train/Test ë°ì´í„° ë¶„í• 
X_xgb = df_train_xgb.drop(columns=["ID", target_col], errors="ignore")
X_test_xgb = df_test_xgb.drop(columns=["ID"], errors="ignore")

class_weights = {0: 0.2583, 1: 0.7417}  # ì‹¤íŒ¨(0) -> 0.25, ì„±ê³µ(1) -> 0.75

positive_ct = ((y == 1).sum())
negative_ct = ((y == 0).sum())
scale_pos_weight = negative_ct/positive_ct

# ğŸ”¹ 6. Optunaë¥¼ í™œìš©í•œ CatBoost & XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
def objective(trial):
    print(f"ğŸ”„ í˜„ì¬ {trial.number + 1}ë²ˆì§¸ íŠœë‹ ì§„í–‰ ì¤‘...")
    
    # âœ… XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°
    xgb_params = {
        "n_estimators": trial.suggest_int("xgb_n_estimators", 500, 2000),
        "max_depth": trial.suggest_int("xgb_max_depth", 3, 10),
        "learning_rate": trial.suggest_loguniform("xgb_learning_rate", 0.005, 0.1),
        "subsample": trial.suggest_uniform("xgb_subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_uniform("xgb_colsample_bytree", 0.5, 1.0),
        "gamma": trial.suggest_loguniform("xgb_gamma", 1e-3, 10.0),
        "min_child_weight": trial.suggest_int("xgb_min_child_weight", 1, 10),
        "reg_lambda": trial.suggest_loguniform("xgb_reg_lambda", 1e-2, 100.0),
        "reg_alpha": trial.suggest_loguniform("xgb_reg_alpha", 1e-2, 100.0),
        "eval_metric": "auc",
        "missing": np.nan,
        "random_state": 42,
        "use_label_encoder": False,
        "early_stopping_rounds": 50,
        "scale_pos_weight": scale_pos_weight
    }

    # âœ… bootstrap_type ê°’ì„ ë¨¼ì € ê³ ì •
    bootstrap_type = trial.suggest_categorical("bootstrap_type", ["Bernoulli", "Poisson", "Bayesian"])
    
    # âœ… CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°
    cat_params = {
        "iterations": trial.suggest_int("cat_iterations", 500, 3000),
        "depth": trial.suggest_int("cat_depth", 4, 10),
        "learning_rate": trial.suggest_loguniform("cat_learning_rate", 0.005, 0.1),
        "l2_leaf_reg": trial.suggest_loguniform("cat_l2_leaf_reg", 1.0, 50.0),
        "border_count": trial.suggest_int("cat_border_count", 16, 64),
        "grow_policy": trial.suggest_categorical("cat_grow_policy", ["SymmetricTree", "Lossguide", "Depthwise"]),
        "bootstrap_type": bootstrap_type,
        "class_weights": [class_weights[0], class_weights[1]],
        "random_seed": 42,
        "task_type": "GPU",
        "eval_metric": "Logloss",
        "loss_function": "Logloss",
        "verbose": 0
    }

    # âœ… Stratified K-Fold ê²€ì¦ ì ìš©
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    auc_scores = []

    for train_idx, valid_idx in kf.split(X, y):
        X_train_xgb, X_valid_xgb = X_xgb.iloc[train_idx], X_xgb.iloc[valid_idx]
        X_train_cat, X_valid_cat = X_cat.iloc[train_idx], X_cat.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        model_xgb = XGBClassifier(**xgb_params)
        model_cat = CatBoostClassifier(**cat_params)

        model_xgb.fit(X_train_xgb, y_train, eval_set=[(X_valid_xgb, y_valid)], verbose=0)
        model_cat.fit(X_train_cat, y_train, eval_set=(X_valid_cat, y_valid), cat_features=cat_features, verbose=0)

        y_pred_xgb = model_xgb.predict_proba(X_valid_xgb)[:, 1]
        y_pred_cat = model_cat.predict_proba(X_valid_cat)[:, 1]

        y_pred_ensemble = (y_pred_xgb + y_pred_cat) / 2
        auc_scores.append(roc_auc_score(y_valid, y_pred_ensemble))

    auc_score = np.mean(auc_scores)

    # âœ… í˜„ì¬ trialì˜ AUC ì ìˆ˜ ì¶œë ¥
    print(f"âœ… {trial.number + 1}ë²ˆì§¸ íŠœë‹ ì™„ë£Œ! ROC-AUC: {auc_score:.6f}")

    # âœ… ìµœê³  ì ìˆ˜ ê°±ì‹  ì—¬ë¶€ í™•ì¸ ë° ì¶œë ¥
    if trial.number == 0 or auc_score > study.best_value:
        print(f"ğŸ”¥ New Best Trial Found! AUC: {auc_score:.6f} (Previous Best: {study.best_value if trial.number > 0 else 'None'})")

    return auc_score

# ğŸ”¹ 7. Optuna ì‹¤í–‰ (ìµœì í™”)
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

# ğŸ”¹ 8. ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
best_params = study.best_params

# âœ… XGBoost ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
best_xgb_params = {k.replace("xgb_", ""): v for k, v in best_params.items() if "xgb_" in k}

# âœ… CatBoost ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
best_cat_params = {k.replace("cat_", ""): v for k, v in best_params.items() if "cat_" in k}

# âœ… CatBoost ì¶”ê°€ ì„¤ì • (í•„ìˆ˜ ì„¤ì •ê°’ ì¶”ê°€)
best_cat_params["task_type"] = "GPU"
best_cat_params["eval_metric"] = "Logloss"
best_cat_params["loss_function"] = "Logloss"
best_cat_params["verbose"] = 100

# âœ… ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì €ì¥
with open("best_xgb_params.pkl", "wb") as f:
    pickle.dump(best_xgb_params, f)
with open("best_cat_params.pkl", "wb") as f:
    pickle.dump(best_cat_params, f)

print(f"ğŸ“ ìµœì í™”ëœ XGBoost & CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ êµ¬ë¶„ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
