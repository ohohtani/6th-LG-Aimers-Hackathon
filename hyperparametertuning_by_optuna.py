import numpy as np
import pandas as pd
import optuna
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split


file_path_train = "/content/train_processed.csv"
df_train = pd.read_csv(file_path_train)

# ğŸ”¹ 'ID' ì»¬ëŸ¼ ì œê±° (Train)
if "ID" in df_train.columns:
    df_train.drop(columns=["ID"], inplace=True)

# 6ï¸âƒ£ Train ë°ì´í„° ì¤€ë¹„
X = df_train.drop(columns=["ì„ì‹  ì„±ê³µ ì—¬ë¶€"])
y = df_train["ì„ì‹  ì„±ê³µ ì—¬ë¶€"]

# 7ï¸âƒ£ Train ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, ê²€ì¦ 20%)
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# 9ï¸âƒ£ Test ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°)
file_path_test = "/content/test_processed.csv"
df_test = pd.read_csv(file_path_test)

# ğŸ”¹ 'ID' ì»¬ëŸ¼ ì œê±° (Test)
if "ID" in df_test.columns:
    df_test.drop(columns=["ID"], inplace=True)

def objective(trial):
    # íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 500, 2000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.1),
        "subsample": trial.suggest_uniform("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.5, 1.0),
        "colsample_bylevel": trial.suggest_uniform("colsample_bylevel", 0.5, 1.0),
        "gamma": trial.suggest_loguniform("gamma", 1e-3, 10.0),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-2, 100.0),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-2, 100.0),
        "eval_metric": "auc",
        "tree_method": "hist",      # GPU ì‚¬ìš©ì„ ìœ„í•´ "hist" ì‚¬ìš© (gpu_histëŠ” deprecated)
        "device": "cuda",           # GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •
        "missing": np.nan,
        "random_state": 42,
        "use_label_encoder": False,
        "early_stopping_rounds": 30,
    }

    # XGBClassifier ìƒì„± í›„ í•™ìŠµ (verbose=Falseë¡œ í•™ìŠµ ì§„í–‰ìƒí™© ìƒëµ)
    model = XGBClassifier(**params)
    model.fit(
        X_train, y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False
    )
    # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ì–‘ì„± í´ë˜ìŠ¤)
    preds = model.predict_proba(X_valid)[:, 1]
    # ROC-AUC ìŠ¤ì½”ì–´ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ maximize ë°©í–¥ìœ¼ë¡œ ìµœì í™”)
    score = roc_auc_score(y_valid, preds)
    return score

# Optuna ìŠ¤í„°ë”” ìƒì„± (ëª©í‘œ: ROC-AUC ìµœëŒ€í™”)
study = optuna.create_study(direction="maximize")
# ì˜ˆì‹œ: 50ë²ˆì˜ trialë¡œ íŠœë‹ (n_trialsëŠ” í•„ìš”ì— ë”°ë¼ ì¡°ì •)
study.optimize(objective, n_trials=50)

print("Best trial:")
best_trial = study.best_trial
print(f"  ROC-AUC: {best_trial.value:.4f}")
print("  Best hyperparameters:")
for key, value in best_trial.params.items():
    print(f"    {key}: {value}")
"""
ìœ„ ì½”ë“œë¡œ ì–»ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤.
ì•½ 380ë²ˆ (í•œì‹œê°„,ì½”ë© gpu T4)ì •ë„ ëŒë ¸ìŠµë‹ˆë‹¤. ì‹œí—Œë‹˜ì´ ì˜¬ë ¤ì£¼ì‹  ì½”ë“œì—ì„œ ë”°ë¡œ ì²˜ë¦¬ ì•ˆí•œ ìƒíƒœì—ì„œ ëŒë¦¬ë©´ validation 0.74005 ë‚˜ì˜µë‹ˆë‹¤.
Best trial:
  ROC-AUC: 0.7401
  Best hyperparameters:
    n_estimators: 1460
    max_depth: 5
    learning_rate: 0.013528250043705115
    subsample: 0.5004290707020331
    colsample_bytree: 0.8090419117285462
    colsample_bylevel: 0.8224549845420901
    gamma: 0.0021286966552009406
    min_child_weight: 1
    reg_lambda: 13.10737878511581
    reg_alpha: 1.4380716740432997

"""
