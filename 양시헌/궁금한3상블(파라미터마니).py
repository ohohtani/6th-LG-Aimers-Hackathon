# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import pandas as pd
import optuna
import pickle
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from ngboost import NGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.calibration import CalibratedClassifierCV

# âœ… ë°ì´í„° ê²€ì¦ í•¨ìˆ˜
def validate_data(X_train, X_test, cat_features):
    print("\nğŸš€ [ë°ì´í„° ê²€ì¦ ì‹œì‘]")
    train_cat_cols = X_train.select_dtypes(include=["object"]).columns.tolist()
    test_cat_cols = X_test.select_dtypes(include=["object"]).columns.tolist()

    if train_cat_cols:
        print(f"ğŸ” í•™ìŠµ ë°ì´í„° ë¬¸ìí˜• ì»¬ëŸ¼: {train_cat_cols}")
    else:
        print("âœ… í•™ìŠµ ë°ì´í„°ì— ë¬¸ìí˜• ì»¬ëŸ¼ ì—†ìŒ.")
    
    if test_cat_cols:
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¬¸ìí˜• ì»¬ëŸ¼: {test_cat_cols}")
    else:
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë¬¸ìí˜• ì»¬ëŸ¼ ì—†ìŒ.")

    if X_train.isnull().values.any():
        print("âŒ í•™ìŠµ ë°ì´í„° ê²°ì¸¡ì¹˜ ë°œê²¬!")
    else:
        print("âœ… í•™ìŠµ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì—†ìŒ.")
        
    if X_test.isnull().values.any():
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°ì¸¡ì¹˜ ë°œê²¬!")
    else:
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì—†ìŒ.")
    
    if list(X_train.columns) != list(X_test.columns):
        print("âŒ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì»¬ëŸ¼ ë¶ˆì¼ì¹˜!")
        raise ValueError("ğŸš« ì»¬ëŸ¼ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²° í•„ìš”.")
    else:
        print("âœ… í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì»¬ëŸ¼ ì¼ì¹˜.")
    print("âœ… [ë°ì´í„° ê²€ì¦ ì™„ë£Œ]\n")

# ğŸ”¹ 1. ë°ì´í„° ë¡œë“œ
df_train = pd.read_csv("train3_updated.csv")
df_test = pd.read_csv("test3_updated.csv")
df_sample_submission = pd.read_csv("sample_submission.csv")
test_ids = df_sample_submission["ID"]

# ğŸ”¹ 2. Train ë°ì´í„° ì¤€ë¹„
target_col = "ì„ì‹  ì„±ê³µ ì—¬ë¶€"
X = df_train.drop(columns=["ID", target_col], errors="ignore")
y = df_train[target_col]
X_test = df_test.drop(columns=["ID"], errors="ignore")

# ğŸ”¹ 3. ë²”ì£¼í˜• ë³€ìˆ˜ í™•ì¸
cat_features = X.select_dtypes(include=["object"]).columns.tolist()

# âœ… ë°ì´í„° ê²€ì¦
validate_data(X, X_test, cat_features)

# ğŸ”¹ 4. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •
class_weights = {0: 0.2583, 1: 0.7417}

# âœ… ê³µí†µ ê²€ì¦ K-Fold ì„¤ì •
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# -------------------------------
# ğŸ”¹ 5. CatBoost íŠœë‹
# -------------------------------
def catboost_objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 1000, 3000),  # ë²”ìœ„ í™•ì¥
        "depth": trial.suggest_int("depth", 4, 12),  # ê¹Šì´ ë²”ìœ„ í™•ì¥
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.1),  # í•™ìŠµë¥  ë²”ìœ„ ì„¸ë°€í™”
        "l2_leaf_reg": trial.suggest_loguniform("l2_leaf_reg", 0.1, 100.0),  # L2 ì •ê·œí™” ë²”ìœ„ í™•ì¥
        "border_count": trial.suggest_int("border_count", 32, 128),  # ê²½ê³„ ìˆ˜ ë²”ìœ„ í™•ì¥
        "grow_policy": trial.suggest_categorical("grow_policy", ["SymmetricTree", "Lossguide", "Depthwise"]),
        "bootstrap_type": trial.suggest_categorical("bootstrap_type", ["Bernoulli", "Poisson", "Bayesian"]),
        "class_weights": [class_weights[0], class_weights[1]],
        "random_seed": 42,
        "task_type": "GPU",
        "eval_metric": "AUC",
        "loss_function": "Logloss",
        "verbose": 0
    }

    if params["bootstrap_type"] in ["Bernoulli", "Poisson"]:
        params["subsample"] = trial.suggest_uniform("subsample", 0.6, 1.0)  # ì„œë¸Œìƒ˜í”Œë§ ë²”ìœ„ ì¡°ì •

    aucs = []
    for train_idx, valid_idx in kf.split(X, y):
        model = CatBoostClassifier(**params)
        model.fit(X.iloc[train_idx], y.iloc[train_idx], eval_set=(X.iloc[valid_idx], y.iloc[valid_idx]),
                  cat_features=cat_features, early_stopping_rounds=100, verbose=0)
        preds = model.predict_proba(X.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_cat = optuna.create_study(direction="maximize")
study_cat.optimize(catboost_objective, n_trials=50)  # ì‹œë„ íšŸìˆ˜ ì¦ê°€
best_params_cat = study_cat.best_params
print("âœ… CatBoost ìµœì  íŒŒë¼ë¯¸í„°:", best_params_cat)

# -------------------------------
# ğŸ”¹ 6. LightGBM íŠœë‹
# -------------------------------
def lgbm_objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 1000, 3000),  # ë²”ìœ„ í™•ì¥
        "max_depth": trial.suggest_int("max_depth", 3, 15),  # ê¹Šì´ ë²”ìœ„ í™•ì¥
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.0001, 0.1),  # í•™ìŠµë¥  ë²”ìœ„ ì„¸ë°€í™”
        "num_leaves": trial.suggest_int("num_leaves", 31, 511),  # ìì˜ ìˆ˜ ë²”ìœ„ í™•ì¥
        "subsample": trial.suggest_uniform("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.5, 1.0),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-3, 100.0),  # ì•ŒíŒŒ ë²”ìœ„ í™•ì¥
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-3, 100.0),  # ëŒë‹¤ ë²”ìœ„ í™•ì¥
        "random_state": 42
    }

    aucs = []
    for train_idx, valid_idx in kf.split(X, y):
        model = LGBMClassifier(**params)
        model.fit(X.iloc[train_idx], y.iloc[train_idx],
                  eval_set=[(X.iloc[valid_idx], y.iloc[valid_idx])],
                  early_stopping_rounds=50, verbose=0)
        preds = model.predict_proba(X.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_lgbm = optuna.create_study(direction="maximize")
study_lgbm.optimize(lgbm_objective, n_trials=50)  # ì‹œë„ íšŸìˆ˜ ì¦ê°€
best_params_lgbm = study_lgbm.best_params
print("âœ… LightGBM ìµœì  íŒŒë¼ë¯¸í„°:", best_params_lgbm)

# -------------------------------
# ğŸ”¹ 7. NGBoost íŠœë‹
# -------------------------------
def ngboost_objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 1000, 3000),  # ë²”ìœ„ í™•ì¥
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.0001, 0.1),  # í•™ìŠµë¥  ë²”ìœ„ ì„¸ë°€í™”
        "minibatch_frac": trial.suggest_uniform("minibatch_frac", 0.3, 1.0),  # ë¯¸ë‹ˆë°°ì¹˜ ë¹„ìœ¨ ë²”ìœ„ ì¡°ì •
        "col_sample": trial.suggest_uniform("col_sample", 0.3, 1.0),  # ì»¬ëŸ¼ ìƒ˜í”Œë§ ë²”ìœ„ ì¡°ì •
        "natural_gradient": trial.suggest_categorical("natural_gradient", [True, False]),
        "random_state": 42
    }

    aucs = []
    for train_idx, valid_idx in kf.split(X, y):
        model = NGBClassifier(**params)
        model.fit(X.iloc[train_idx], y.iloc[train_idx])
        preds = model.predict_proba(X.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_ngb = optuna.create_study(direction="maximize")
study_ngb.optimize(ngboost_objective, n_trials=50)  # ì‹œë„ íšŸìˆ˜ ì¦ê°€
best_params_ngb = study_ngb.best_params
print("âœ… NGBoost ìµœì  íŒŒë¼ë¯¸í„°:", best_params_ngb)

# -------------------------------
# ğŸ”¹ 8. ëª¨ë¸ í•™ìŠµ
# -------------------------------
# CatBoost ëª¨ë¸ í•™ìŠµ
cat_model = CatBoostClassifier(**best_params_cat)
cat_model.fit(X, y, cat_features=cat_features, verbose=100)

# LightGBM ëª¨ë¸ í•™ìŠµ
lgbm_model = LGBMClassifier(**best_params_lgbm)
lgbm_model.fit(X, y)

# NGBoost ëª¨ë¸ í•™ìŠµ
ngb_model = NGBClassifier(**best_params_ngb)
ngb_model.fit(X, y)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
cat_preds = cat_model.predict_proba(X_test)[:, 1]
lgbm_preds = lgbm_model.predict_proba(X_test)[:, 1]
ngb_preds = ngb_model.predict_proba(X_test)[:, 1]

# ì†Œí”„íŠ¸ ë³´íŒ…
final_preds = (cat_preds + lgbm_preds + ngb_preds) / 3

# -------------------------------
# ğŸ”¹ 10. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš©
# -------------------------------
calibrated_preds = np.zeros(len(X_test))
for train_idx, valid_idx in kf.split(X, y):
    model = CatBoostClassifier(**best_params_cat)
    model.fit(X.iloc[train_idx], y.iloc[train_idx], cat_features=cat_features, verbose=0)

    calibrator = CalibratedClassifierCV(base_estimator=model, method='sigmoid', cv='prefit')
    calibrator.fit(X.iloc[valid_idx], y.iloc[valid_idx])
    calibrated_preds += calibrator.predict_proba(X_test)[:, 1] / kf.n_splits

# -------------------------------
# ğŸ”¹ 11. ì œì¶œ íŒŒì¼ ìƒì„±
# -------------------------------
df_submission = pd.DataFrame({"ID": test_ids, "probability": calibrated_preds})
df_submission.to_csv("ensemble_final.csv", index=False)
print("âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: 'ensemble_final.csv'")
