# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import pandas as pd
import optuna
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from ngboost import NGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.calibration import CalibratedClassifierCV

# âœ… ë°ì´í„° ê²€ì¦ í•¨ìˆ˜
def validate_data(X_train, X_test, cat_features):
    print("\nğŸš€ [ë°ì´í„° ê²€ì¦ ì‹œì‘]")
    train_cat_cols = X_train.select_dtypes(include=["object"]).columns.tolist()
    test_cat_cols = X_test.select_dtypes(include=["object"]).columns.tolist()

    if train_cat_cols:
        print(f"ğŸ” í•™ìŠµ ë°ì´í„° ë¬¸ìí˜• ì»¬ëŸ¼: {train_cat_cols}")
    else:
        print("âœ… í•™ìŠµ ë°ì´í„°ì— ë¬¸ìí˜• ì»¬ëŸ¼ ì—†ìŒ.")
    
    if test_cat_cols:
        print(f"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¬¸ìí˜• ì»¬ëŸ¼: {test_cat_cols}")
    else:
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë¬¸ìí˜• ì»¬ëŸ¼ ì—†ìŒ.")

    if X_train.isnull().values.any():
        print("âŒ í•™ìŠµ ë°ì´í„° ê²°ì¸¡ì¹˜ ë°œê²¬!")
    else:
        print("âœ… í•™ìŠµ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì—†ìŒ.")
        
    if X_test.isnull().values.any():
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°ì¸¡ì¹˜ ë°œê²¬!")
    else:
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ ì—†ìŒ.")
    
    if list(X_train.columns) != list(X_test.columns):
        print("âŒ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì»¬ëŸ¼ ë¶ˆì¼ì¹˜!")
        raise ValueError("ğŸš« ì»¬ëŸ¼ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²° í•„ìš”.")
    else:
        print("âœ… í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì»¬ëŸ¼ ì¼ì¹˜.")
    print("âœ… [ë°ì´í„° ê²€ì¦ ì™„ë£Œ]\n")

# ğŸ”¹ 1. ë°ì´í„° ë¡œë“œ
df_train = pd.read_csv("train3_updated.csv")
df_test = pd.read_csv("test3_updated.csv")
df_sample_submission = pd.read_csv("sample_submission.csv")
test_ids = df_sample_submission["ID"]

# ğŸ”¹ 2. Train ë°ì´í„° ì¤€ë¹„
target_col = "ì„ì‹  ì„±ê³µ ì—¬ë¶€"
X = df_train.drop(columns=["ID", target_col], errors="ignore")
y = df_train[target_col]
X_test = df_test.drop(columns=["ID"], errors="ignore")

# ğŸ”¹ 3. ë²”ì£¼í˜• ë³€ìˆ˜ í™•ì¸
cat_features = X.select_dtypes(include=["object"]).columns.tolist()

# âœ… ë°ì´í„° ê²€ì¦
validate_data(X, X_test, cat_features)

# ğŸ”¹ 4. ìˆ«ìí˜• ë°ì´í„°ë§Œ ì¶”ì¶œ (LightGBM & NGBoostìš©)
numeric_features = X.select_dtypes(exclude=["object"]).columns.tolist()
X_numeric = X[numeric_features]
X_test_numeric = X_test[numeric_features]

# ğŸ”¹ 5. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •
class_weights = {0: 0.2583, 1: 0.7417}

# âœ… ê³µí†µ ê²€ì¦ K-Fold ì„¤ì •
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# -------------------------------
# ğŸ”¹ 6. CatBoost íŠœë‹
# -------------------------------
def catboost_objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 500, 2000),
        "depth": trial.suggest_int("depth", 4, 10),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.005, 0.1),
        "l2_leaf_reg": trial.suggest_loguniform("l2_leaf_reg", 1.0, 50.0),
        "border_count": trial.suggest_int("border_count", 16, 64),
        "grow_policy": trial.suggest_categorical("grow_policy", ["SymmetricTree", "Lossguide", "Depthwise"]),
        "bootstrap_type": trial.suggest_categorical("bootstrap_type", ["Bernoulli", "Poisson", "Bayesian"]),
        "class_weights": [class_weights[0], class_weights[1]],
        "random_seed": 42,
        "task_type": "GPU",
        "eval_metric": "AUC",
        "loss_function": "Logloss",
        "verbose": 0
    }

    if params["bootstrap_type"] in ["Bernoulli", "Poisson"]:
        params["subsample"] = trial.suggest_uniform("subsample", 0.7, 1.0)

    aucs = []
    for train_idx, valid_idx in kf.split(X, y):
        model = CatBoostClassifier(**params)
        model.fit(X.iloc[train_idx], y.iloc[train_idx], eval_set=(X.iloc[valid_idx], y.iloc[valid_idx]),
                  cat_features=cat_features, early_stopping_rounds=100, verbose=0)
        preds = model.predict_proba(X.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_cat = optuna.create_study(direction="maximize")
study_cat.optimize(catboost_objective, n_trials=20)
best_params_cat = study_cat.best_params
print("âœ… CatBoost ìµœì  íŒŒë¼ë¯¸í„°:", best_params_cat)

# -------------------------------
# ğŸ”¹ 7. LightGBM íŠœë‹ (ìˆ«ìí˜•ë§Œ ì‚¬ìš©)
# -------------------------------
def lgbm_objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 500, 2000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.1),
        "num_leaves": trial.suggest_int("num_leaves", 31, 255),
        "subsample": trial.suggest_uniform("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.5, 1.0),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-2, 10.0),
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-2, 10.0),
        "random_state": 42
    }

    aucs = []
    for train_idx, valid_idx in kf.split(X_numeric, y):
        model = LGBMClassifier(**params)
        model.fit(X_numeric.iloc[train_idx], y.iloc[train_idx],
                  eval_set=[(X_numeric.iloc[valid_idx], y.iloc[valid_idx])],
                  early_stopping_rounds=50, verbose=0)
        preds = model.predict_proba(X_numeric.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_lgbm = optuna.create_study(direction="maximize")
study_lgbm.optimize(lgbm_objective, n_trials=20)
best_params_lgbm = study_lgbm.best_params
print("âœ… LightGBM ìµœì  íŒŒë¼ë¯¸í„°:", best_params_lgbm)

# -------------------------------
# ğŸ”¹ 8. NGBoost íŠœë‹ (ìˆ«ìí˜•ë§Œ ì‚¬ìš©)
# -------------------------------
def ngboost_objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 500, 2000),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.1),
        "minibatch_frac": trial.suggest_uniform("minibatch_frac", 0.5, 1.0),
        "col_sample": trial.suggest_uniform("col_sample", 0.5, 1.0),
        "natural_gradient": trial.suggest_categorical("natural_gradient", [True, False]),
        "random_state": 42
    }

    aucs = []
    for train_idx, valid_idx in kf.split(X_numeric, y):
        model = NGBClassifier(**params)
        model.fit(X_numeric.iloc[train_idx], y.iloc[train_idx])
        preds = model.predict_proba(X_numeric.iloc[valid_idx])[:, 1]
        aucs.append(roc_auc_score(y.iloc[valid_idx], preds))
    return np.mean(aucs)

study_ngb = optuna.create_study(direction="maximize")
study_ngb.optimize(ngboost_objective, n_trials=20)
best_params_ngb = study_ngb.best_params
print("âœ… NGBoost ìµœì  íŒŒë¼ë¯¸í„°:", best_params_ngb)

# -------------------------------
# ğŸ”¹ 9. ëª¨ë¸ í•™ìŠµ
# -------------------------------
# âœ… CatBoost (ë²”ì£¼í˜• + ìˆ«ìí˜•)
cat_model = CatBoostClassifier(**best_params_cat)
cat_model.fit(X, y, cat_features=cat_features, verbose=100)

# âœ… LightGBM (ìˆ«ìí˜•ë§Œ ì‚¬ìš©)
lgbm_model = LGBMClassifier(**best_params_lgbm)
lgbm_model.fit(X_numeric, y)

# âœ… NGBoost (ìˆ«ìí˜•ë§Œ ì‚¬ìš©)
ngb_model = NGBClassifier(**best_params_ngb)
ngb_model.fit(X_numeric, y)

# -------------------------------
# ğŸ”¹ 10. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë° **ê°€ì¤‘ì¹˜ ì ìš© ì•™ìƒë¸”**
# -------------------------------
cat_preds = cat_model.predict_proba(X_test)[:, 1]
lgbm_preds = lgbm_model.predict_proba(X_test_numeric)[:, 1]
ngb_preds = ngb_model.predict_proba(X_test_numeric)[:, 1]

# âœ… CatBoostì— ê°€ì¤‘ì¹˜ 0.5, LightGBMê³¼ NGBoostì— ê°ê° 0.25 ì ìš©
final_preds = (0.5 * cat_preds) + (0.25 * lgbm_preds) + (0.25 * ngb_preds)

# -------------------------------
# ğŸ”¹ 11. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì ìš© (CatBoost ê¸°ì¤€)
# -------------------------------
calibrated_preds = np.zeros(len(X_test))
for train_idx, valid_idx in kf.split(X, y):
    model = CatBoostClassifier(**best_params_cat)
    model.fit(X.iloc[train_idx], y.iloc[train_idx], cat_features=cat_features, verbose=0)

    calibrator = CalibratedClassifierCV(base_estimator=model, method='sigmoid', cv='prefit')
    calibrator.fit(X.iloc[valid_idx], y.iloc[valid_idx])
    calibrated_preds += calibrator.predict_proba(X_test)[:, 1] / kf.n_splits

# -------------------------------
# ğŸ”¹ 12. ì œì¶œ íŒŒì¼ ìƒì„±
# -------------------------------
df_submission = pd.DataFrame({"ID": test_ids, "probability": calibrated_preds})
df_submission.to_csv("ensemble_final.csv", index=False)
print("âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: 'ensemble_final.csv'")
