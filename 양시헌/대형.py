import numpy as np
import pandas as pd
import optuna
import torch
from xgboost import XGBClassifier
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

# ğŸ”¹ 1. ë°ì´í„° ë¡œë“œ
file_path_train = "train_processed.csv"
file_path_test = "test_processed.csv"
sample_submission_path = "sample_submission.csv"

df_train = pd.read_csv(file_path_train)
df_test = pd.read_csv(file_path_test)
df_sample_submission = pd.read_csv(sample_submission_path)

# ğŸ”¹ 2. 'ID' ì»¬ëŸ¼ ìœ ì§€ (sample_submissionì„ ìœ„í•´ í•„ìš”)
test_ids = df_sample_submission["ID"]

# ğŸ”¹ 3. Train ë°ì´í„° ì¤€ë¹„
X = df_train.drop(columns=["ID", "ì„ì‹  ì„±ê³µ ì—¬ë¶€"], errors="ignore")  
y = df_train["ì„ì‹  ì„±ê³µ ì—¬ë¶€"]

# ğŸ”¹ 4. Train ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, ê²€ì¦ 20%)
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ”¹ 5. XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (GPU ì‚¬ìš©)
def optimize_xgb(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 500, 2000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_loguniform("learning_rate", 0.005, 0.05),
        "subsample": trial.suggest_uniform("subsample", 0.3, 1.0),
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.3, 1.0),
        "gamma": trial.suggest_loguniform("gamma", 1e-6, 1e-1),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 20),
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1.0, 20.0),
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-5, 1e-1),
        "eval_metric": "auc",
        "tree_method": "gpu_hist",  # ğŸ”¥ GPU ì‚¬ìš©
        "use_label_encoder": False,
        "random_state": 42
    }
    model = XGBClassifier(**params)
    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)
    preds = model.predict_proba(X_valid)[:, 1]
    return roc_auc_score(y_valid, preds)

study_xgb = optuna.create_study(direction="maximize")
study_xgb.optimize(optimize_xgb, n_trials=50)
best_xgb_params = study_xgb.best_params

# ğŸ”¹ 6. TabNet í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (GPU ì‚¬ìš©)
def optimize_tabnet(trial):
    params = {
        "optimizer_params": dict(lr=trial.suggest_loguniform("learning_rate", 0.005, 0.05)),
        "batch_size": trial.suggest_int("batch_size", 128, 512),
        "virtual_batch_size": trial.suggest_int("virtual_batch_size", 64, 256),
    }
    model = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=params["optimizer_params"], device_name="cuda")  # ğŸ”¥ GPU ì‚¬ìš©
    model.fit(X_train.values, y_train.values, eval_set=[(X_valid.values, y_valid.values)], eval_metric=["auc"], max_epochs=100, patience=10, batch_size=params["batch_size"], virtual_batch_size=params["virtual_batch_size"])
    preds = model.predict_proba(X_valid.values)[:, 1]
    return roc_auc_score(y_valid, preds)

study_tabnet = optuna.create_study(direction="maximize")
study_tabnet.optimize(optimize_tabnet, n_trials=50)
best_tabnet_params = study_tabnet.best_params

# ğŸ”¹ 7. Classic Model (SVM, CatBoost) ìµœì í™” ë° í•™ìŠµ
model_svm = SVC(probability=True, random_state=42)  # ğŸš« SVMì€ CPUì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥
model_catboost = CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.01, random_state=42, verbose=False, task_type="GPU")  # ğŸ”¥ GPU ì‚¬ìš©

# ğŸ”¹ 8. ëª¨ë¸ í•™ìŠµ
model_xgb = XGBClassifier(**best_xgb_params)
model_xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)

model_tabnet = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=best_tabnet_params["learning_rate"]), device_name="cuda")  # ğŸ”¥ GPU ì‚¬ìš©
model_tabnet.fit(X_train.values, y_train.values, eval_set=[(X_valid.values, y_valid.values)], eval_metric=["auc"], max_epochs=100, patience=10, batch_size=best_tabnet_params["batch_size"], virtual_batch_size=best_tabnet_params["virtual_batch_size"])

model_svm.fit(X_train, y_train)  # ğŸš« CPU ì „ìš©
model_catboost.fit(X_train, y_train)

# ğŸ”¹ 9. ì˜ˆì¸¡ê°’ ìƒì„±
valid_preds_xgb = model_xgb.predict_proba(X_valid)[:, 1]
valid_preds_tabnet = model_tabnet.predict_proba(X_valid.values)[:, 1]
valid_preds_svm = model_svm.predict_proba(X_valid)[:, 1]
valid_preds_catboost = model_catboost.predict_proba(X_valid)[:, 1]

# ğŸ”¹ 10. ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” (Validation Loss ê¸°ë°˜)
weights = {
    "xgb": 1 / (roc_auc_score(y_valid, valid_preds_xgb) + 1e-8),
    "tabnet": 1 / (roc_auc_score(y_valid, valid_preds_tabnet) + 1e-8),
    "svm": 1 / (roc_auc_score(y_valid, valid_preds_svm) + 1e-8),
    "catboost": 1 / (roc_auc_score(y_valid, valid_preds_catboost) + 1e-8),
}
total_weight = sum(weights.values())

valid_preds_ensemble = (
    (valid_preds_xgb * weights["xgb"]) +
    (valid_preds_tabnet * weights["tabnet"]) +
    (valid_preds_svm * weights["svm"]) +
    (valid_preds_catboost * weights["catboost"])
) / total_weight

roc_auc = roc_auc_score(y_valid, valid_preds_ensemble)
print(f"ğŸ“Š ìµœì¢… ì•™ìƒë¸” ROC-AUC Score: {roc_auc:.4f}")

# ğŸ”¹ 11. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
X_test = df_test.drop(columns=["ID"], errors="ignore")
test_preds_xgb = model_xgb.predict_proba(X_test)[:, 1]
test_preds_tabnet = model_tabnet.predict_proba(X_test.values)[:, 1]
test_preds_svm = model_svm.predict_proba(X_test)[:, 1]
test_preds_catboost = model_catboost.predict_proba(X_test)[:, 1]

test_preds_ensemble = (
    (test_preds_xgb * weights["xgb"]) +
    (test_preds_tabnet * weights["tabnet"]) +
    (test_preds_svm * weights["svm"]) +
    (test_preds_catboost * weights["catboost"])
) / total_weight

# ğŸ”¹ 12. ìµœì¢… ê²°ê³¼ ì €ì¥
df_submission = pd.DataFrame({"ID": test_ids, "probability": test_preds_ensemble})
df_submission.to_csv("xgboost_tabnet_svm_catboost_ensemble.csv", index=False)

print(f"ğŸš€ ìµœì í™”ëœ ì•™ìƒë¸” ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")
