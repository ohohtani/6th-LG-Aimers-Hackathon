import os
import numpy as np
import pandas as pd
import optuna
import pickle
import matplotlib.pyplot as plt
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder

# ğŸ”¹ 1. GPU 1ë²ˆ ì‚¬ìš© ê°•ì œ ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # NVIDIA GeForce RTX (GPU 1ë²ˆ) ê°•ì œ ì‚¬ìš©

# ğŸ”¹ 2. ë°ì´í„° ë¡œë“œ
file_path_train = "train3_updated.csv"
file_path_test = "test3_updated.csv"
sample_submission_path = "sample_submission.csv"

df_train = pd.read_csv(file_path_train)
df_test = pd.read_csv(file_path_test)
df_sample_submission = pd.read_csv(sample_submission_path)

# ğŸ”¹ 3. 'ID' ì»¬ëŸ¼ ìœ ì§€
test_ids = df_sample_submission["ID"]

# ğŸ”¹ 4. Train ë°ì´í„° ì¤€ë¹„
target_col = "ì„ì‹  ì„±ê³µ ì—¬ë¶€"
if target_col not in df_train.columns:
    raise ValueError(f"âŒ '{target_col}' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

X = df_train.drop(columns=["ID", target_col], errors="ignore")
y = df_train[target_col]

# ğŸ”¹ 5. ë²”ì£¼í˜• ë³€ìˆ˜ í™•ì¸
cat_features = X.select_dtypes(include=["object"]).columns.tolist()

# ğŸ”¹ 6. XGBoostë¥¼ ìœ„í•œ Label Encoding
X_xgb = X.copy()
X_test_xgb = df_test.drop(columns=["ID"], errors="ignore")

if cat_features:
    combined_df = pd.concat([X[cat_features], X_test_xgb[cat_features]])
    for col in cat_features:
        le = LabelEncoder()
        combined_df[col] = le.fit_transform(combined_df[col])
    
    X_xgb[cat_features] = combined_df.iloc[:len(X)][cat_features]
    X_test_xgb[cat_features] = combined_df.iloc[len(X):][cat_features]

# ğŸ”¹ 7. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •
class_weights = {0: 0.25, 1: 0.75}

# ğŸ”¹ 8. Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (K-Fold ì ìš©)
auc_history = []

def objective(trial):
    """Optunaë¥¼ ì´ìš©í•œ XGBoost & CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
    params_xgb = {
        "n_estimators": trial.suggest_int("xgb_n_estimators", 500, 3000),
        "max_depth": trial.suggest_int("xgb_max_depth", 3, 10),
        "learning_rate": trial.suggest_loguniform("xgb_learning_rate", 0.005, 0.1),
        "subsample": trial.suggest_uniform("xgb_subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_uniform("xgb_colsample_bytree", 0.5, 1.0),
        "reg_lambda": trial.suggest_loguniform("xgb_reg_lambda", 1.0, 50.0),
        "reg_alpha": trial.suggest_loguniform("xgb_reg_alpha", 0.01, 10.0),
        "eval_metric": "auc",
        "random_state": 10,
        "early_stopping_rounds": 100,
        "tree_method": "hist",
        "device": "cuda"  # GPU ì‚¬ìš©
    }

    params_cat = {
        "iterations": trial.suggest_int("cat_iterations", 500, 3000),
        "depth": trial.suggest_int("cat_depth", 4, 10),
        "learning_rate": trial.suggest_loguniform("cat_learning_rate", 0.005, 0.1),
        "l2_leaf_reg": trial.suggest_loguniform("cat_l2_leaf_reg", 1.0, 50.0),
        "border_count": trial.suggest_int("cat_border_count", 16, 64),
        "grow_policy": trial.suggest_categorical("cat_grow_policy", ["SymmetricTree", "Lossguide", "Depthwise"]),
        "class_weights": [class_weights[0], class_weights[1]],
        "random_seed": 10,
        "task_type": "GPU",
        "devices": "1",  # GPU 1ë²ˆ ì‚¬ìš© ì„¤ì •
        "eval_metric": "AUC",
        "loss_function": "Logloss",
        "verbose": 0
    }

    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)
    auc_scores = []

    for train_idx, valid_idx in kf.split(X, y):
        X_train_xgb, X_valid_xgb = X_xgb.iloc[train_idx], X_xgb.iloc[valid_idx]
        X_train_cat, X_valid_cat = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        # XGBoost ëª¨ë¸ í•™ìŠµ
        model_xgb = XGBClassifier(**params_xgb)
        model_xgb.fit(X_train_xgb, y_train, eval_set=[(X_valid_xgb, y_valid)], verbose=0)

        # CatBoost ëª¨ë¸ í•™ìŠµ
        model_cat = CatBoostClassifier(**params_cat)
        model_cat.fit(X_train_cat, y_train, eval_set=(X_valid_cat, y_valid), cat_features=cat_features, verbose=0)

        # Soft Voting
        preds_xgb = model_xgb.predict_proba(X_valid_xgb)[:, 1]
        preds_cat = model_cat.predict_proba(X_valid_cat)[:, 1]
        preds_ensemble = (preds_xgb + preds_cat) / 2

        auc_scores.append(roc_auc_score(y_valid, preds_ensemble))

    mean_auc = np.mean(auc_scores)
    auc_history.append(mean_auc)

    print(f"ğŸŸ¢ Trial {trial.number} | AUC Score: {mean_auc:.5f}")

    return mean_auc

# ğŸ”¹ 9. Optuna ì‹¤í–‰
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# ğŸ”¹ 10. ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ (íŒŒì¼ë¡œ ì €ì¥)
best_params = study.best_params
with open("ensemble_hyperparameters.pkl", "wb") as f:
    pickle.dump(best_params, f)

print("ğŸ“ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ 'ensemble_hyperparameters.pkl' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ğŸ”¹ 11. ìµœì  ëª¨ë¸ í•™ìŠµ
best_params_xgb = {k.replace("xgb_", ""): v for k, v in best_params.items() if k.startswith("xgb_")}
best_params_cat = {k.replace("cat_", ""): v for k, v in best_params.items() if k.startswith("cat_")}

model_xgb = XGBClassifier(**best_params_xgb, tree_method="hist", device="cuda")
model_cat = CatBoostClassifier(**best_params_cat, cat_features=cat_features, task_type="GPU", devices='1')

model_xgb.fit(X_xgb, y)
model_cat.fit(X, y)

# ğŸ”¹ 12. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
test_preds_xgb = model_xgb.predict_proba(X_test_xgb)[:, 1]
test_preds_cat = model_cat.predict_proba(df_test.drop(columns=["ID"], errors="ignore"))[:, 1]
test_preds_ensemble = (test_preds_xgb + test_preds_cat) / 2

# ğŸ”¹ 13. ì œì¶œ íŒŒì¼ ìƒì„±
df_submission = pd.DataFrame({"ID": test_ids, "probability": test_preds_ensemble})
submission_file_path = "ensemble_best2.csv"
df_submission.to_csv(submission_file_path, index=False)

print(f"âœ… ìµœì í™”ëœ ê²°ê³¼ê°€ '{submission_file_path}' ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
